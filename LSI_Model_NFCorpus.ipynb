{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JS_ls0yEpzE0"
      },
      "source": [
        "# Load CFCorpus dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 471,
      "metadata": {
        "id": "NYMntKqZ_ZDS"
      },
      "outputs": [],
      "source": [
        "# store each doc in a seperated file\n",
        "doc_id_dict = {}\n",
        "raw_document = []\n",
        "\n",
        "with open('./NFCorpus/test.docs', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    index = 0\n",
        "    for line in lines:\n",
        "        id, doc = line.split('\\t')\n",
        "        \n",
        "        doc_id_dict[id] = index\n",
        "        index += 1\n",
        "        raw_document.append(doc)\n",
        "        \n",
        "    f.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "O1Q2HBkDpzea"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 472,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /Users/macos/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import os\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "term_freq = {}# số lần xuất hiện của các term cả tài liệu Cranfield\n",
        "doc_freq = {}# số tài liệu xuất hiện của các term mà ta xét\n",
        "ps = PorterStemmer()\n",
        "noise = string.punctuation + '0123456789'\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "def preProcessing(content):\n",
        "  # loại bỏ ký hiệu và số tách thành mãng\n",
        "  words = content.translate(str.maketrans(\"\", \"\", noise)).split()\n",
        "  # stemming\n",
        "  words = [ps.stem(i) for i in words]\n",
        "  # loại bỏ stopwords\n",
        "  words = [word for word in words if word not in stop_words]\n",
        "  return words\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 473,
      "metadata": {},
      "outputs": [],
      "source": [
        "document = []\n",
        "for doc in raw_document:\n",
        "    document.append(preProcessing(doc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 474,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['statin',\n",
              " 'breast',\n",
              " 'cancer',\n",
              " 'surviv',\n",
              " 'nationwid',\n",
              " 'cohort',\n",
              " 'studi',\n",
              " 'finland',\n",
              " 'abstract',\n",
              " 'recent',\n",
              " 'studi',\n",
              " 'suggest',\n",
              " 'statin',\n",
              " 'establish',\n",
              " 'drug',\n",
              " 'group',\n",
              " 'prevent',\n",
              " 'cardiovascular',\n",
              " 'mortal',\n",
              " 'delay',\n",
              " 'prevent',\n",
              " 'breast',\n",
              " 'cancer',\n",
              " 'recurr',\n",
              " 'effect',\n",
              " 'diseasespecif',\n",
              " 'mortal',\n",
              " 'remain',\n",
              " 'unclear',\n",
              " 'evalu',\n",
              " 'risk',\n",
              " 'breast',\n",
              " 'cancer',\n",
              " 'death',\n",
              " 'statin',\n",
              " 'user',\n",
              " 'populationbas',\n",
              " 'cohort',\n",
              " 'breast',\n",
              " 'cancer',\n",
              " 'patient',\n",
              " 'studi',\n",
              " 'cohort',\n",
              " 'includ',\n",
              " 'newli',\n",
              " 'diagnos',\n",
              " 'breast',\n",
              " 'cancer',\n",
              " 'patient',\n",
              " 'finland',\n",
              " 'num',\n",
              " 'num',\n",
              " 'num',\n",
              " 'case',\n",
              " 'identifi',\n",
              " 'finnish',\n",
              " 'cancer',\n",
              " 'registri',\n",
              " 'inform',\n",
              " 'statin',\n",
              " 'diagnosi',\n",
              " 'obtain',\n",
              " 'nation',\n",
              " 'prescript',\n",
              " 'databas',\n",
              " 'cox',\n",
              " 'proport',\n",
              " 'hazard',\n",
              " 'regress',\n",
              " 'method',\n",
              " 'estim',\n",
              " 'mortal',\n",
              " 'statin',\n",
              " 'user',\n",
              " 'statin',\n",
              " 'timedepend',\n",
              " 'variabl',\n",
              " 'total',\n",
              " 'num',\n",
              " 'particip',\n",
              " 'statin',\n",
              " 'median',\n",
              " 'followup',\n",
              " 'num',\n",
              " 'year',\n",
              " 'diagnosi',\n",
              " 'rang',\n",
              " 'num',\n",
              " 'num',\n",
              " 'year',\n",
              " 'num',\n",
              " 'particip',\n",
              " 'die',\n",
              " 'num',\n",
              " 'num',\n",
              " 'due',\n",
              " 'breast',\n",
              " 'cancer',\n",
              " 'adjust',\n",
              " 'age',\n",
              " 'tumor',\n",
              " 'characterist',\n",
              " 'treatment',\n",
              " 'select',\n",
              " 'postdiagnost',\n",
              " 'prediagnost',\n",
              " 'statin',\n",
              " 'lower',\n",
              " 'risk',\n",
              " 'breast',\n",
              " 'cancer',\n",
              " 'death',\n",
              " 'hr',\n",
              " 'num',\n",
              " 'num',\n",
              " 'ci',\n",
              " 'num',\n",
              " 'num',\n",
              " 'hr',\n",
              " 'num',\n",
              " 'num',\n",
              " 'ci',\n",
              " 'num',\n",
              " 'num',\n",
              " 'risk',\n",
              " 'decreas',\n",
              " 'postdiagnost',\n",
              " 'statin',\n",
              " 'affect',\n",
              " 'healthi',\n",
              " 'adher',\n",
              " 'bia',\n",
              " 'greater',\n",
              " 'likelihood',\n",
              " 'die',\n",
              " 'cancer',\n",
              " 'patient',\n",
              " 'discontinu',\n",
              " 'statin',\n",
              " 'associ',\n",
              " 'dosedepend',\n",
              " 'observ',\n",
              " 'lowdoseshortterm',\n",
              " 'dose',\n",
              " 'timedepend',\n",
              " 'surviv',\n",
              " 'benefit',\n",
              " 'prediagnost',\n",
              " 'statin',\n",
              " 'user',\n",
              " 'suggest',\n",
              " 'causal',\n",
              " 'effect',\n",
              " 'evalu',\n",
              " 'clinic',\n",
              " 'trial',\n",
              " 'test',\n",
              " 'statin',\n",
              " 'effect',\n",
              " 'surviv',\n",
              " 'breast',\n",
              " 'cancer',\n",
              " 'patient']"
            ]
          },
          "execution_count": 474,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "document[0]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YuIrBJ5Ap5kg"
      },
      "source": [
        "# LSI MODEL"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "A5ZtIQa7p-nP"
      },
      "source": [
        "## Decomposition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 475,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import modules\n",
        "from gensim import corpora\n",
        "from gensim.models import LsiModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.models import TfidfModel\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def prepare_corpus(doc_clean):\n",
        "    \"\"\"\n",
        "    Input  : clean document\n",
        "    Purpose: create term dictionary of our corpus and convert the list of documents (corpus) into Document Term Matrix using tf-idf weighting\n",
        "    Output : term dictionary and Document Term Matrix\n",
        "    \"\"\"\n",
        "    # Creating the term dictionary of our corpus, where every unique term is assigned an index.\n",
        "    dictionary = corpora.Dictionary(doc_clean)\n",
        "\n",
        "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
        "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
        "\n",
        "    # Apply tf-idf weighting to the Document Term Matrix\n",
        "    tfidf = TfidfModel(doc_term_matrix)\n",
        "    doc_term_matrix_tfidf = tfidf[doc_term_matrix]\n",
        "\n",
        "    return dictionary, doc_term_matrix_tfidf\n",
        "\n",
        "# Pass doc_clean as the parameter to prepare_corpus function\n",
        "dictionary, doc_term_matrix = prepare_corpus(document)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 476,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert doc_term_matrix into a dense numpy array\n",
        "num_terms = len(dictionary)\n",
        "num_docs = len(doc_term_matrix)\n",
        "matrix = np.zeros((num_terms, num_docs), dtype=np.float32)  # Transpose the dimensions\n",
        "\n",
        "for i, doc in enumerate(doc_term_matrix):\n",
        "    for term_id, term_freq in doc:\n",
        "        matrix[term_id, i] = term_freq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 477,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(19424, 3162)"
            ]
          },
          "execution_count": 477,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 478,
      "metadata": {},
      "outputs": [],
      "source": [
        "def svd_decomposition(matrix):\n",
        "    \"\"\"\n",
        "    Perform Singular Value Decomposition (SVD) on the given matrix M.\n",
        "\n",
        "    Parameters:\n",
        "        matrix: Input matrix of shape (m x n)\n",
        "\n",
        "    Returns:\n",
        "        S: Matrix of left singular vectors of shape (m x n)\n",
        "        sigma: Singular values as a 1-D array of length min(n, n)\n",
        "        U: Transpose of the matrix of right singular vectors of shape (n x n)\n",
        "    \"\"\"\n",
        "    # Perform SVD\n",
        "    S, sigma, U_t = np.linalg.svd(matrix, full_matrices=False)\n",
        "\n",
        "\n",
        "    return S, sigma, U_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 479,
      "metadata": {},
      "outputs": [],
      "source": [
        "S, sigma, U_t = svd_decomposition(matrix)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xukpJ69JqAmW"
      },
      "source": [
        "## Find optimal k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 480,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start=2, step=3):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, stop, step):\n",
        "        model = LsiModel(doc_term_matrix, num_topics= num_topics, id2word = dictionary)  # train model\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "    return model_list, coherence_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 481,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_graph(doc_clean,start, stop, step):\n",
        "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
        "    model_list, coherence_values = compute_coherence_values(dictionary, doc_term_matrix,doc_clean,\n",
        "                                                            stop, start, step)\n",
        "    # Show graph\n",
        "    x = range(start, stop, step)\n",
        "    plt.plot(x, coherence_values)\n",
        "    plt.xlabel(\"Number of Topics\")\n",
        "    plt.ylabel(\"Coherence score\")\n",
        "    plt.legend((\"coherence_values\"), loc='best')\n",
        "    plt.show()\n",
        "\n",
        "start,stop,step=100,1000,100\n",
        "plot_graph(document, start, stop, step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 482,
      "metadata": {},
      "outputs": [],
      "source": [
        "k = 450"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xIfvWEfTqZDX"
      },
      "source": [
        "## Reduce dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 483,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reduce_dimension(S, sigma, UT, k):\n",
        "    \"\"\"\n",
        "    Reduce the dimensionality of matrices U, S, and V based on a given value of k.\n",
        "\n",
        "    Parameters:\n",
        "        U: Matrix of left singular vectors of shape (m x n)\n",
        "        S: Singular values as a 1-D array of length n\n",
        "        V: Matrix of right singular vectors of shape (n x n)\n",
        "        k: Number of singular values/vectors to retain\n",
        "\n",
        "    Returns:\n",
        "        U_red: Reduced matrix U of shape (m x k)\n",
        "        S_red: Reduced array of singular values of length k\n",
        "        V_red: Reduced matrix V of shape (k x n)\n",
        "    \"\"\"\n",
        "    S_red = S[:, :k]\n",
        "    sigma_red = sigma[:k]\n",
        "    UT_red = UT[:k, :]\n",
        "    return S_red, sigma_red, UT_red"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 484,
      "metadata": {},
      "outputs": [],
      "source": [
        "S_red, sigma_red, UT_red = reduce_dimension(S, sigma, U_t, k)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "g7gSzGYcqgOU"
      },
      "source": [
        "# Calculate query matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 485,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "# store each doc in a seperated file\n",
        "raw_query = {}\n",
        "\n",
        "with open('./NFCorpus/test.all.queries', 'r') as f:\n",
        "    lines = f.readlines()    \n",
        "    for line in lines:\n",
        "        id, query = line.split('\\t')        \n",
        "                \n",
        "        raw_query[id] = query\n",
        "        \n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 486,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sort the queries based on the numeric part of the IDs in ascending order\n",
        "sorted_queries = [content for _, content in sorted(raw_query.items(), key=lambda x: int(re.findall(r'\\d+', x[0])[0]))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 487,
      "metadata": {},
      "outputs": [],
      "source": [
        "queries = []\n",
        "for query in sorted_queries:        \n",
        "    preProcess_query = preProcessing(query)\n",
        "    query_new = \" \".join(preProcess_query)\n",
        "    queries.append(query_new)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 499,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cholesterol statin drug caus breast cancer doe breast cancer cholesterol mani potenti mechan cholesterol boost breast cancer growth exampl bodi make estrogen correl increas risk breast cancer cholesterol also packag cholesterol ldl see video cholesterol feed breast cancer cell appear increas cancer prolifer decreas patient surviv cholesterol major compon “ lipid raft ” compar normal counterpart cancer cell higher level cholesterolrich lipid raft plasma membran may import cancer cell surviv may serv human cancer develop term tumor migrat invas elev level cholesterolrich lipid raft found breast cancer cell hypothesi reduc blood cholesterol level “ may disrupt lipid raft format therebi inhibit breast cancer develop ” thi suggest cholesterol target may use cancer therapi control laboratori experi shown phytosterol seed nut dietari relev level appear inhibit growth sever type tumor cell includ breast cancer cell includ estrogenreceptor neg estrogenreceptor posit cancer therapeut implic “ plantbas diet rich phytosterol may offer protect develop breast cancer ” cours ’ make lot money pumpkin seed research look cholesterollow statin drug thi studi highlight video statin cholesterol drug invas breast cancer petri dish work look promis popul studi shown mix result studi show women statin decreas breast cancer risk show increas risk show associ rel shortterm studi though “ longterm ” statin use wa defin mostli three five year breast cancer take decad grow one studi look ten year statin use onli includ case given increas statin use past decad fact ’ commonli prescrib taken everi day rest women ’ live studi publish date onli limit abil evalu impact long durat use better figur thi one four women thi countri drug chang public studi includ thousand breast cancer case long term statin user — women take statin ten year — doubl risk major type breast cancer invas ductal carcinoma invas lobular carcinoma onc women get breast cancer though recent studi finland uk suggest statin use may improv surviv number one killer women heart diseas breast cancer still need bring cholesterol level might way get benefit cholesterol reduct without risk plantbas diet shown lower ldlcholesterol within coupl week equival standard cholesterol lower statin drug without ani breast cancer risk drug go statin remark safe still present rare seriou side effect men women statin muscl toxic lower cholesterol without drug ’ pure question diet lower cholesterol lower intak three thing tran fat satur fat cholesterol toler upper intak zero tran fat found tran fat meat dairi cholesterol found predominantli egg egg cholesterol patent fals mislead claim also food particularli adept lower cholesterol level topic doe scienc say sprout green like like sprout sunflow green mani peopl onlin health commun rave therapeut nutrit valu eat sproutgreen well sprout raw bean seed thi actual accur document scienc assumpt mayb even wrong see sunflow sprout someth probabl left ground want design grow big sunflow ’ babi sprout contain antinutri deter human stop ’ lifecycl grow foot flower beauti plant mani video sprout inform raw food ’ suggest brenda davi ’ book “ becom raw ” – perhap referenc book rw food guest post paleo diet toodo thi impli woman ’ statin develop breast cancer put statin increas chanc surviv word statin next breastcanc drug hi great video mention statin breast cancer risk may realli help thank posthttp wwwmedscapecom viewarticl src emailthi studi present american societi clinic oncologist meet thi year said statin lower risk cancer death type cancer studi say statin ’ affect cancer incidencehi sandra ’ seem open link mind resend perhap need updat inform seem concern take statin ’ associ increas risk breast cancerit ’ help breast cancer heart diseas diseas sensit person topic mani found difficult suggest healthier diet even refer blog like thi medic site pill ‘ ’ thank knowledg share encourag stay openmind true stanceread thi today http wwwhsphharvardedu news pressreleas treatingmoreadultswithstatinswouldbecosteffectivewaytoboosthearthealth utm sourc silverpopmail utm medium email utm campaign harvard chan school updat juli – friend utm content yike better costeffect way help public reduc heart diseas risk semi rhetor question thank link veri interestinga friend mine wa diagnos al last year one possibl caus diseas inhi case year use statin neurologist took medic immediatelyi ’ confus dietari cholesterol appear impli place dietari cholestrol avoid caus increas serum cholesterol yet even tcollin campbel acknowlog thi case understand anim protein caus bodi overproduc cholesterol mayb ’ matter anim protein come lot cholesterol sat fat anyway health recommend would would appreci clarif thi thank great work hi marti let tri help satur fat boost cholesterol much dietari cholesterol dietari cholesterol still rais blood cholesterol slightli thi british medic journal blog explain dr greger comment end state “ inde institut medicin set toler upper intak limit cholesterol “ becaus ani intak level abov energi increas ldl cholesterol concentr ” optim intak may inde zero heart diseas remain lead killer uk us trumbo pr shimakawa toler upper intak level tran fat satur fat cholesterol nutr rev ” hi video optim cholesterol level may also help anim protein detriment health excess uncertain effect cholesterol would agre statement “ mayb ’ matter anim protein come lot cholesterol sat fat anyway ” cholesterol promot cancer “ excess ” cholesterol ’ curiou becaus bodi creat cholesterol need excess may get diet problem ani cholesterol ’ problem woman commerci seem happi act like lotteri find “ crestor ” danc street high five everybodi way friend congratul great newsi normal cholesterol ’ never happythes call “ danc fat ss ” … drug make happi … usual corpor swill ch ’ concern low read elderli live longer higher ch … real issu oxid ch … blood levelsi wonder mani take statin cholesterol high due iodin defici iodineresearchcomanoth translat accomplish portugues – http nffocoempaticonet estatinascausamcancrodamama wow ’ sure like see drop cholesterol … ’ vegan x year level still higher usual recommend levelsdr greger say statin remark safe consid drug mention proviso occasion caus muscl pain diabet ’ read coupl studi indic take statin anywher doubl rate contract diabet take statin take low dose statin less diabet breast cancer cancer cancer surviv cardiovascular diseas cardiovascular health cholesterol estrogen heart diseas heart health ldl cholesterol nut phytosterol plantbas diet plasma membran pumpkin seed seed side effect statin vegan vegetarian women health'"
            ]
          },
          "execution_count": 499,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "queries[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 489,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cal_vector_K(S_red, sigma_red):\n",
        "  vector_K = np.dot(S_red, np.diag(sigma_red))\n",
        "\n",
        "  return vector_K\n",
        "\n",
        "def cal_vector_D(sigma_red, UT_red):\n",
        "  vector_D = np.dot(np.diag(sigma_red), UT_red)\n",
        "\n",
        "  return vector_D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 490,
      "metadata": {},
      "outputs": [],
      "source": [
        "D = cal_vector_D(sigma_red, UT_red)\n",
        "K = cal_vector_K(S_red, sigma_red)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 491,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cal_vector_querry(query, K):\n",
        "    query_terms = query.split()\n",
        "    query_vector = np.zeros(K.shape[1])  # Initialize query vector as zeros\n",
        "\n",
        "    for term in query_terms:\n",
        "        if term in dictionary.token2id:\n",
        "            term_index = dictionary.token2id[term]\n",
        "            query_vector += K[term_index]\n",
        "\n",
        "    return query_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 492,
      "metadata": {},
      "outputs": [],
      "source": [
        "list_q = []\n",
        "for query in queries:\n",
        "  q = cal_vector_querry(query, K)\n",
        "  list_q.append(q)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMCFIBYPp8Qp"
      },
      "source": [
        "# CALCULATE SIMILARITY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 493,
      "metadata": {
        "id": "RAIlDDRGp40C"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def calculate_cosine_similarity(query, document):\n",
        "    # Compute the dot product of the query and document vectors\n",
        "    dot_product = np.dot(query, document)\n",
        "\n",
        "    # Compute the norms of the query and document vectors\n",
        "    query_norm = norm(query)\n",
        "    document_norm = norm(document)\n",
        "\n",
        "    # Compute the cosine similarity\n",
        "    similarity = dot_product / (query_norm * document_norm)\n",
        "\n",
        "    return similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 494,
      "metadata": {},
      "outputs": [],
      "source": [
        "predict_results = []\n",
        "k = 12\n",
        "for q_idx in range(0, len(list_q)):\n",
        "  doc_similarity = []\n",
        "\n",
        "  for doc_idx in range(0, len(document)):\n",
        "    sim = calculate_cosine_similarity(list_q[q_idx], D[:, doc_idx])\n",
        "    doc_similarity.append((doc_idx, sim))\n",
        "\n",
        "  ranked_doc = sorted(doc_similarity, key=lambda x: x[1], reverse=True)\n",
        "  top_k_docs = [item[0] for item in ranked_doc[:k]]\n",
        "\n",
        "  predict_results.append(top_k_docs)\n",
        "  # predict_results.append(ranked_doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 495,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "323"
            ]
          },
          "execution_count": 495,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(predict_results)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EVALUATING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 496,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_file = \"./NFCorpus/test.2-1-0.qrel\"  # Replace with the actual path to your result file\n",
        "\n",
        "query_doc_ids = {}\n",
        "with open(result_file, \"r\") as file:\n",
        "    for line in file:\n",
        "        line = line.strip().split(\"\\t\")\n",
        "        query_id = line[0]\n",
        "        doc_id = line[2]\n",
        "        if query_id in query_doc_ids:\n",
        "            query_doc_ids[query_id].append(doc_id)\n",
        "        else:\n",
        "            query_doc_ids[query_id] = [doc_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 497,
      "metadata": {},
      "outputs": [],
      "source": [
        "real_results = []\n",
        "for res in query_doc_ids:\n",
        "    converted_doc_id = []    \n",
        "    for doc_id in query_doc_ids[res]:\n",
        "        converted_doc_id.append(doc_id_dict[doc_id])\n",
        "    \n",
        "    real_results.append(converted_doc_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 498,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.0811179325724781, 0.08044672827609692, 0.13235294117647062)"
            ]
          },
          "execution_count": 498,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Kiểm tra tài liều đó có nằm trong tài liệu thực không truy vẫn thực không ---> danh sách true false của tài liệu truy vấn của mình(OK)\n",
        "# tính từng cặp r và p ----> bảng r p\n",
        "# tính AP của từng câu truy vấn\n",
        "# cuối cùng tính MAP\n",
        "# lấy dữ liệu RES để tính MAP nhưng chưa xong--------\n",
        "\n",
        "\n",
        "#Hàm lấy index true\n",
        "def index_true(lis):\n",
        "  return [i for i in range(len(lis)) if lis[i] == True]\n",
        "\n",
        "# Hàm này dùng để tính R và P của các câu truy vấn\n",
        "def RP(real_results, predict_results):\n",
        "\n",
        "#Tính R và P của từng câu truy vấn\n",
        "  check_results = [[predict_results[i][j] in real_results[i] for j in range(len(predict_results[i]))] for i in range(len(predict_results))]# kiểm tra xem câu tìm được có đúng kết quả không\n",
        "  index_true_results = [index_true(i) for i in check_results]#đây là index của các dự đoán đúng dùng để tính độ chính xác Precision\n",
        "  len_results = [len(i) for i in real_results]#số kết quả \"thực\" tìm được của mỗi câu truy vấn để tính độ phủ Recall\n",
        "  R_P_results = [[((j+1)/len_results[i], (j+1)/(index_true_results[i][j]+1)) for j in range(len(index_true_results[i]))] for i in range(len(index_true_results))]#kết quả R và P của mỗi câu truy vấn\n",
        "  return R_P_results\n",
        "\n",
        "def Recall(real_results, predict_results):\n",
        "  check_results = [[predict_results[i][j] in real_results[i] for j in range(len(predict_results[i]))] for i in range(len(predict_results))]# kiểm tra xem câu tìm được có đúng kết quả không\n",
        "  len_results = [len(i) for i in real_results]#số kết quả \"thực\" tìm được của mỗi câu truy vấn\n",
        "  recall = [(check_results[i].count(True))/len_results[i] for i in range(len(check_results)) if check_results[i]]\n",
        "  return sum(recall)/len(real_results)\n",
        "\n",
        "def Precision(real_results, predict_results):\n",
        "  check_results = [[predict_results[i][j] in real_results[i] for j in range(len(predict_results[i]))] for i in range(len(predict_results))]# kiểm tra xem câu tìm được có đúng kết quả không\n",
        "  len_predict = [len(i) for i in predict_results]#số kết quả \"dự đoán\" tìm được của mỗi câu truy vấn\n",
        "  precision = [(check_results[i].count(True))/len_predict[i] for i in range(len(check_results)) if check_results[i]]\n",
        "  return sum(precision)/len(real_results)\n",
        "\n",
        "R_P_results = RP(real_results, predict_results)\n",
        "\n",
        "\n",
        "\n",
        "# đây là 11 điểm nội suy của TREC\n",
        "R = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
        "\n",
        "#Tính AP của từng câu truy vấn từ R_P_results\n",
        "# P nội suy là Max(Pr') trong đó r'>=r(đang xét)\n",
        "\n",
        "#hàm check_r dùng để kiểm tra và lấy ra index của r trong rp\n",
        "# Lấy index của R đang xét để tìm giá trị P max từ index đó trở về sau\n",
        "def check_r(r, rp):\n",
        "  for i in range(len(rp)):\n",
        "    if rp[i][0]>=r:\n",
        "      return i\n",
        "  return -1\n",
        "# Lấy P max\n",
        "def max_P(check, rp):\n",
        "  maxx = 0\n",
        "  for i in rp[check:]:\n",
        "    if i[1]>maxx:\n",
        "      maxx = i[1]\n",
        "  return maxx\n",
        "\n",
        "\n",
        "def MAP_11(R_P_results, R):\n",
        "\n",
        "  AP_results = []\n",
        "  for i in range(len(R_P_results)):\n",
        "    ap = 0\n",
        "    for j in range(11):\n",
        "      Check = check_r(R[j],R_P_results[i])\n",
        "      if Check != -1:\n",
        "        ap = ap + max_P(Check,R_P_results[i])\n",
        "    AP_results.append(ap/11)\n",
        "\n",
        "  MAP = sum(AP_results)/225\n",
        "  return MAP\n",
        "\n",
        "MAP_11(R_P_results, R), Recall(real_results, predict_results), Precision(real_results, predict_results)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
